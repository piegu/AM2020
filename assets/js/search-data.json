{
  
    
        "post0": {
            "title": "Neural Networks A Brief History",
            "content": "Home | Curso | . Redes neurais: uma breve história . Fonte: tradução de “Your deep learning journey” do livro Fastai (Jeremy Howard e Sylvain Gugger). . Modelo matemático de um neurônio artificial . Em 1943, Warren McCulloch, um neurofisiologista, e Walter Pitts, (logician), se uniram para desenvolver um modelo matemático de um neurônio artificial. Eles declararam que: . Devido ao caráter “tudo ou nada” da atividade nervosa, os eventos neurais e as relações entre eles podem ser tratados por meio da lógica proposicional. Verifica-se que o comportamento de toda rede pode ser descrito nesses termos. (Pitts and McCulloch; A Logical Calculus of the Ideas Immanent in Nervous Activity) . Eles perceberam que um modelo simplificado de um neurônio real poderia ser representado usando adição e limiares simples, como mostrado aqui: . . O Perceptron: a capacidade de aprender . Frank Rosenblatt desenvolveu em 1956 o neurônio artificial para lhe dar a capacidade de aprender. Mais importante ainda, ele trabalhou na construção do primeiro dispositivo que realmente usava esses princípios: The Mark I Perceptron. . Rosenblatt escreveu sobre este trabalho: “estamos prestes a testemunhar o nascimento de tal máquina - uma máquina capaz de perceber, reconhecer e identificar seu entorno sem nenhum treinamento ou controle humano”. O Perceptron foi construído e conseguiu reconhecer formas simples. . Livro “Perceptrons”: início do primeiro inverno da IA . Um professor do MIT chamado Marvin Minsky (que ficou atrás de Rosenblatt na mesma escola!) junto com Seymour Papert escreveu um livro, chamado “Perceptrons”, sobre a invenção de Rosenblatt. Eles mostraram que uma única camada desses dispositivos não conseguiu aprender algumas funções matemáticas simples e críticas (como o XOR). . No mesmo livro, eles também mostraram que o uso de várias camadas dos dispositivos permitiria resolver essas limitações. Infelizmente, apenas a primeira dessas idéias foi amplamente reconhecida, como resultado da comunidade acadêmica global quase inteiramente desistindo de redes neurais pelas próximas duas décadas. . Redes neurais . Talvez o trabalho mais crucial em redes neurais nos últimos 50 anos seja o multi-volumes Parallel Distributed Processing (PDP), lançado em 1986 pela MIT Press. Capítulo 1 estabelece uma esperança semelhante ao mostrado por Rosenblatt: . … as pessoas são mais espertos do que os computadores de hoje porque o cérebro emprega uma arquitetura computacional básico que é mais adequado para lidar com um aspecto central das informações naturais processamento de tarefas que as pessoas são tão boas. … introduziremos uma estrutura computacional para modelar processos cognitivos que parece … mais próxima do que outras estruturas ao estilo de computação, como pode ser feito pelo cérebro. (PDP, capítulo 1) . A premissa que o PDP está usando aqui é que os programas de computador tradicionais funcionam de maneira muito diferente dos cérebros, e pode ser por isso que os programas de computador (naquele momento) foram tão ruins em fazer coisas que os cérebros acham fáceis (como reconhecer objetos nas fotos). Os autores afirmam que a abordagem PDP é “mais próxima do que outras estruturas” de como o cérebro funciona e, portanto, pode ser mais capaz de lidar com esse tipo de tarefa. . De fato, a abordagem apresentada no PDP é muito semelhante à usada nas redes neurais atuais. O livro definia “Parallel Distributed Processing” como exigindo: . Um conjunto de processing units . | Um estado de ativação (state of activation) . | Uma função de saída (output function) para cada unidade . | Um padrão de conectividade (pattern of connectivity) entre unidades . | Uma regra de propagação (propagation rule) para propagar padrões de atividades através da rede de conectividades . | Uma regra de ativação (activation rule) para combinar as entradas que colidem com uma unidade com o estado atual dessa unidade para produzir um novo nível de ativação para a unidade. . | Uma regra de aprendizado (learning rule) segundo a qual os padrões de conectividade são modificados pela experiência. . | Um ambiente (environment )no qual o sistema deve operar. . | Veremos no Fastai Book que redes neurais lidam com cada um desses requisitos. . Década de 1980: primeiras aplicações de NN . Na década de 1980, a maioria dos modelos foi construída com uma segunda camada de neurônios, evitando assim o problema identificado por Minsky (esse era seu “padrão de conectividade entre unidades”, para usar a estrutura acima). . E, de fato, as redes neurais foram amplamente usadas nas décadas de 80 e 90 para projetos reais e práticos. . No entanto, novamente um equívoco das questões teóricas impediu o campo. Em teoria, adicionar apenas uma camada extra de neurônios era suficiente para permitir que qualquer modelo matemático se aproximasse dessas redes neurais, mas, na prática, essas redes eram muitas vezes grandes e lentas demais para serem úteis. . Mais camadas! . Embora os pesquisadores tenham mostrado há 30 anos que, para obter um bom desempenho prático, você precisa usar ainda mais camadas de neurônios, é apenas na última década que isso tem sido mais amplamente apreciado. . As redes neurais estão finalmente alcançando seu potencial, graças ao entendimento de usar mais camadas e à capacidade aprimorada de fazê-lo, graças a melhorias no hardware do computador, aumentos na disponibilidade de dados e ajustes algorítmicos que permitem que as redes neurais sejam treinadas mais rapidamente e mais facilmente. . Agora temos o que Rosenblatt havia prometido: “uma máquina capaz de perceber, reconhecer e identificar seu entorno sem nenhum treinamento ou controle humano”. .",
            "url": "https://piegu.github.io/AM2020/2020/03/12/neural-networks-a-brief-history.html",
            "relUrl": "/2020/03/12/neural-networks-a-brief-history.html",
            "date": " • Mar 12, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Test Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://piegu.github.io/AM2020/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "Sobre mim",
          "content": "Por sua capacidade de aumentar o potencial humano em muitas áreas, a Inteligência Artificial (IA) altera o papel do ser humano na sociedade com impactos sociais e econômicos muito significativos. Para se preparar para essas mudanças, precisamos de mais cientistas da computação treinados para desenvolver aplicativos de IA, de mais intervenientes que podem definir um quadro ético no uso da IA e de mais profissionais que podem acompanhar a transformação das empresas. . Desde 2016, para contribuir para este triplo objetivo (técnico, ético e de negócio), lançei com outras pessoas várias iniciativas em IA em Brasília: . formação às técnicas de IA: meetup Deep Learning Brasília e Grupo de Estudo do DL em Brasília | palestras e cursos sobre a ética na IA e a IA nos negócios: meetup Inteligência Artificial Brasília | papeis e conferências sobre a IA, o Machine Learning e o Deep Learning (medium.com/@pierre_guillou) | . Sou também: . Consultor em Inteligência Artificial e Deep Learning e trabalha para nama.ai como Lead AI Scientist | Pesquisador Associado ao Laboratório de Inteligência Artificial (AI.Lab) da Faculdade UnB Gama (FGA) | Professor vinculado ao curso de Machine Learning e Deep Learning da Faculdade UnB Gama (FGA) | . Anteriormente, trabalhei principalmente no campo da comunicação digital e acessibilidade à Internet. . Mais informações sobre mim (Pierre Guillou) no meu perfil Linkedin. .",
          "url": "https://piegu.github.io/AM2020/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  
      ,"page2": {
          "title": "Curso",
          "content": "Lição 1 (10/03/2020) - Apresentação da disciplina . Lição 2 (12/03/2020) - Apresentação geral da Inteligência Artificial e Responsabilidades . Redes neurais: uma breve história | .",
          "url": "https://piegu.github.io/AM2020/course/",
          "relUrl": "/course/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

}