<h1 id="redes-neurais-uma-breve-história">Redes neurais: uma breve história</h1>

<p>Fonte: tradução de “<a href="https://github.com/fastai/fastbook/blob/master/01_intro.ipynb"><span class="underline">Your deep learning journey</span></a>” do <a href="https://github.com/fastai/fastbook"><span class="underline">livro Fastai</span></a> (Jeremy Howard e Sylvain Gugger).</p>

<h2 id="modelo-matemático-de-um-neurônio-artificial">Modelo matemático de um neurônio artificial</h2>

<p>Em 1943, <strong>Warren McCulloch</strong>, um neurofisiologista, e <strong>Walter Pitts</strong>, (logician), se uniram para desenvolver <strong>um modelo matemático de um neurônio artificial</strong>. Eles declararam que:</p>

<blockquote>
  <p><em>Devido ao caráter “tudo ou nada” da atividade nervosa, os eventos neurais e as relações entre eles podem ser tratados por meio da lógica proposicional. Verifica-se que o comportamento de toda rede pode ser descrito nesses termos</em>. (Pitts and McCulloch; A Logical Calculus of the Ideas Immanent in Nervous Activity)</p>
</blockquote>

<p>Eles perceberam que <strong>um modelo simplificado de um neurônio real poderia ser representado usando adição e limiares simples,</strong> como mostrado aqui:</p>

<p><img src="assets/img/2020-03-12-neural-networks-a-brief-history/media/image1.png" alt="Natural and artificial neurons" /></p>

<h2 id="o-perceptron-a-capacidade-de-aprender">O Perceptron: a capacidade de aprender</h2>

<p><strong>Frank Rosenblatt</strong> desenvolveu em 1956 o neurônio artificial para lhe dar a <strong>capacidade de aprender</strong>. Mais importante ainda, ele trabalhou na construção do primeiro dispositivo que realmente usava esses princípios: The Mark I <strong>Perceptron</strong>.</p>

<p>Rosenblatt escreveu sobre este trabalho: “<strong>estamos prestes a testemunhar o nascimento de tal máquina - uma máquina capaz de perceber, reconhecer e identificar seu entorno sem nenhum treinamento ou controle humano</strong>”. O Perceptron foi construído e conseguiu reconhecer formas simples.</p>

<h2 id="livro-perceptrons-início-do-primeiro-inverno-da-ia">Livro “Perceptrons”: início do primeiro inverno da IA</h2>

<p>Um professor do MIT chamado <strong>Marvin Minsky</strong> (que ficou atrás de Rosenblatt na mesma escola!) junto com <strong>Seymour Papert</strong> escreveu um <strong>livro, chamado “Perceptrons”</strong>, sobre a invenção de Rosenblatt. <strong>Eles mostraram que uma única camada desses dispositivos não conseguiu aprender algumas funções matemáticas simples e críticas (como o XOR).</strong></p>

<p>No mesmo livro, eles também mostraram que o uso de várias camadas dos dispositivos permitiria resolver essas limitações. Infelizmente, apenas a primeira dessas idéias foi amplamente reconhecida, como resultado da comunidade acadêmica global quase inteiramente desistindo de redes neurais pelas próximas duas décadas.</p>

<h2 id="redes-neurais">Redes neurais</h2>

<p>Talvez o trabalho mais crucial em redes neurais nos últimos 50 anos seja o multi-volumes <em>Parallel Distributed Processing</em> (PDP), lançado em 1986 pela MIT Press. Capítulo 1 estabelece uma esperança semelhante ao mostrado por Rosenblatt:</p>

<blockquote>
  <p><em>… as pessoas são mais espertos do que os computadores de hoje porque o cérebro emprega uma arquitetura computacional básico que é mais adequado para lidar com um aspecto central das informações naturais processamento de tarefas que as pessoas são tão boas. … introduziremos uma estrutura computacional para modelar processos cognitivos que parece … mais próxima do que outras estruturas ao estilo de computação, como pode ser feito pelo cérebro.</em> (PDP, capítulo 1)</p>
</blockquote>

<p>A premissa que o PDP está usando aqui é que <strong>os programas de computador tradicionais funcionam de maneira muito diferente dos cérebros</strong>, e pode ser por isso que os programas de computador (naquele momento) foram tão ruins em fazer coisas que os cérebros acham fáceis (como reconhecer objetos nas fotos). Os autores afirmam que a abordagem PDP é “mais próxima do que outras estruturas” de como o cérebro funciona e, portanto, pode ser mais capaz de lidar com esse tipo de tarefa.</p>

<p>De fato, a abordagem apresentada no PDP é muito semelhante à usada nas redes neurais atuais. O livro definia “Parallel Distributed Processing” como exigindo:</p>

<ol>
  <li>
    <blockquote>
      <p>Um conjunto de <em>processing units</em></p>
    </blockquote>
  </li>
  <li>
    <blockquote>
      <p>Um <em>estado de ativação (state of activation)</em></p>
    </blockquote>
  </li>
  <li>
    <blockquote>
      <p>Uma <em>função de saída</em> (<em>output function)</em> para cada unidade</p>
    </blockquote>
  </li>
  <li>
    <blockquote>
      <p>Um <em>padrão de conectividade</em> (<em>pattern of connectivity)</em> entre unidades</p>
    </blockquote>
  </li>
  <li>
    <blockquote>
      <p>Uma <em>regra de propagação</em> (<em>propagation rule)</em> para propagar padrões de atividades através da rede de conectividades</p>
    </blockquote>
  </li>
  <li>
    <blockquote>
      <p>Uma <em>regra de ativação</em> (<em>activation rule)</em> para combinar as entradas que colidem com uma unidade com o estado atual dessa unidade para produzir um novo nível de ativação para a unidade.</p>
    </blockquote>
  </li>
  <li>
    <blockquote>
      <p>Uma <em>regra de aprendizado</em> (<em>learning rule)</em> segundo a qual os padrões de conectividade são modificados pela experiência.</p>
    </blockquote>
  </li>
  <li>
    <blockquote>
      <p>Um <em>ambiente</em> (<em>environment</em> )no qual o sistema deve operar.</p>
    </blockquote>
  </li>
</ol>

<p>Veremos no Fastai Book que redes neurais lidam com cada um desses requisitos.</p>

<h2 id="década-de-1980-primeiras-aplicações-de-nn">Década de 1980: primeiras aplicações de NN</h2>

<p><strong>Na década de 1980, a maioria dos modelos foi construída com uma segunda camada de neurônios</strong>, evitando assim o problema identificado por Minsky (esse era seu “padrão de conectividade entre unidades”, para usar a estrutura acima).</p>

<p>E, de fato, as redes neurais foram amplamente usadas nas décadas de 80 e 90 para projetos reais e práticos.</p>

<p>No entanto, novamente um equívoco das questões teóricas impediu o campo. <strong>Em teoria, adicionar apenas uma camada extra de neurônios era suficiente para permitir que qualquer modelo matemático se aproximasse dessas redes neurais, mas, na prática, essas redes eram muitas vezes grandes e lentas demais para serem úteis.</strong></p>

<h2 id="mais-camadas">Mais camadas!</h2>

<p>Embora os pesquisadores tenham mostrado há 30 anos que, <strong>para obter um bom desempenho prático, você precisa usar ainda mais camadas de neurônios</strong>, é apenas na última década que isso tem sido mais amplamente apreciado.</p>

<p>As redes neurais estão finalmente alcançando seu potencial, graças ao entendimento de usar mais camadas e à capacidade aprimorada de fazê-lo, graças a melhorias no hardware do computador, aumentos na disponibilidade de dados e ajustes algorítmicos que permitem que as redes neurais sejam treinadas mais rapidamente e mais facilmente.</p>

<p>Agora temos o que Rosenblatt havia prometido: “uma máquina capaz de perceber, reconhecer e identificar seu entorno sem nenhum treinamento ou controle humano”.</p>
