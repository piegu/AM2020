<p>Source: translation from the <a href="https://github.com/fastai/fastbook"><span class="underline">Fastai book</span></a> <a href="https://github.com/fastai/fastbook/blob/master/01_intro.ipynb"><span class="underline">Your deep learning journey</span></a></p>

<h1 id="mathematical-model-of-an-artificial-neuron">Mathematical model of an artificial neuron</h1>

<p>In 1943 <strong>Warren McCulloch</strong>, a neurophysiologist, and <strong>Walter Pitts</strong>, a logician, teamed up to develop <strong>a mathematical model of an artificial neuron</strong>. They declared that:</p>

<blockquote>
  <p><em>Because of the “all-or-none” character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms</em>. (Pitts and McCulloch; A Logical Calculus of the Ideas Immanent in Nervous Activity)</p>
</blockquote>

<p>They realised that <strong>a simplified model of a real neuron could be represented using simple addition and thresholding</strong> as shown here:</p>

<p><img src="assets/img/2020-03-11-FB-1-_-Neural-networks_-a-brief-history/media/image1.png" alt="Natural and artificial neurons" /></p>

<h1 id="the-perceptron-the-ability-to-learn">The Perceptron: the ability to learn</h1>

<p><strong>Frank Rosenblatt</strong> further (1956) developed the artificial neuron to give it the <strong>ability to learn</strong>. Even more importantly, he worked on building the first device that actually used these principles: The Mark I <strong>Perceptron</strong>.</p>

<p>Rosenblatt wrote about this work: “<strong>we are about to witness the birth of such a machine – a machine capable of perceiving, recognizing and identifying its surroundings without any human training or control</strong>”. The perceptron was built, and was able to successfully recognize simple shapes.</p>

<h1 id="perceptrons-start-of-the-first-ai-winter">Perceptrons: start of the first AI Winter</h1>

<p>An MIT professor named <strong>Marvin Minsky</strong> (who was a grade behind Rosenblatt at the same high school!) along with <strong>Seymour Papert</strong> wrote a <strong>book, called “Perceptrons”</strong>, about Rosenblatt’s invention. They showed that a single layer of these devices was unable to learn some simple, critical mathematical functions (such as XOR).</p>

<p>In the same book, they also showed that using multiple layers of the devices would allow these limitations to be addressed. Unfortunately, only the first of these insights was widely recognized, as a result of which the global academic community nearly entirely gave up on neural networks for the next two decades.</p>

<h1 id="neural-networks">Neural Networks</h1>

<p>Perhaps the most pivotal work in neural networks in the last 50 years is the multi-volume <em>Parallel Distributed Processing</em> (PDP), released in 1986 by MIT Press. Chapter 1 lays out a similar hope to that shown by Rosenblatt:</p>

<blockquote>
  <p>: <em>…people are smarter than today’s computers because the brain employs a basic computational architecture that is more suited to deal with a central aspect of the natural information processing tasks that people are so good at. …we will introduce a computational framework for modeling cognitive processes that seems… closer than other frameworks to the style of computation as it might be done by the brain.</em> (PDP, chapter 1)</p>
</blockquote>

<p>The premise that PDP is using here is that <strong>traditional computer programs work very differently to brains</strong>, and that might be why computer programs had (at that point) been so bad at doing things that brains find easy (such as recognizing objects in pictures). The authors claim that the PDP approach is “closer than other frameworks” to how the brain works, and therefore it might be better able to handle these kinds of tasks.</p>

<p>In fact, the approach laid out in PDP is very similar to the approach used in today’s neural networks. The book defined “Parallel Distributed Processing” as requiring:</p>

<ol>
  <li>
    <blockquote>
      <p>A set of <em>processing units</em></p>
    </blockquote>
  </li>
  <li>
    <blockquote>
      <p>A <em>state of activation</em></p>
    </blockquote>
  </li>
  <li>
    <blockquote>
      <p>An <em>output function</em> for each unit</p>
    </blockquote>
  </li>
  <li>
    <blockquote>
      <p>A <em>pattern of connectivity</em> among units</p>
    </blockquote>
  </li>
  <li>
    <blockquote>
      <p>A <em>propagation rule</em> for propagating patterns of activities through the network of connectivities</p>
    </blockquote>
  </li>
  <li>
    <blockquote>
      <p>An <em>activation rule</em> for combining the inputs impinging on a unit with the current state of that unit to produce a new level of activation for the unit</p>
    </blockquote>
  </li>
  <li>
    <blockquote>
      <p>A <em>learning rule</em> whereby patterns of connectivity are modified by experience</p>
    </blockquote>
  </li>
  <li>
    <blockquote>
      <p>An <em>environment</em> within which the system must operate</p>
    </blockquote>
  </li>
</ol>

<p>We will see in this book that modern neural networks handle each of these requirements.</p>

<h1 id="1980s-first-applications-of-nn">1980’s: first applications of NN</h1>

<p>In the 1980’s most models were built with a second layer of neurons, thus avoiding the problem that had been identified by Minsky (this was their “pattern of connectivity among units”, to use the framework above). And indeed, neural networks were widely used during the 80s and 90s for real, practical projects. However, again a misunderstanding of the theoretical issues held back the field. In theory, adding just one extra layer of neurons was enough to allow any mathematical model to be approximated with these neural networks, but in practice such networks were often too big and slow to be useful.</p>

<h1 id="more-layers">More layers!</h1>

<p>Although researchers showed 30 years ago that to get practical good performance you need to use even more layers of neurons, it is only in the last decade that this has been more widely appreciated. Neural networks are now finally living up to their potential, thanks to the understanding to use more layers as well as improved ability to do so thanks to improvements in computer hardware, increases in data availability, and algorithmic tweaks that allow neural networks to be trained faster and more easily.</p>

<p>We now have what Rosenblatt had promised: “a machine capable of perceiving, recognizing and identifying its surroundings without any human training or control”.</p>
